{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44531154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24980288",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('titanic (1).csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b262dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0454159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skmis\\AppData\\Local\\Temp\\ipykernel_27228\\1990016140.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Age'].fillna(data['Age'].median(),inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data['Age'].fillna(data['Age'].median(),inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a53ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  28.0      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611d55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Cabin','Ticket','Name','PassengerId'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5680db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "888         0       3  female  28.0      1      2  23.4500        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3424e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [\"Age\",\"Fare\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c13eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"Survived\"] \n",
    "data.drop(\"Survived\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4131be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skmis\\AppData\\Local\\Temp\\ipykernel_27228\\2507124730.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Embarked'].fillna(data['Embarked'].mode()[0],inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data['Embarked'].fillna(data['Embarked'].mode()[0],inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c776669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
       "0         3  22.0      1      0   7.2500      True       False        True\n",
       "1         1  38.0      1      0  71.2833     False       False       False\n",
       "2         3  26.0      0      0   7.9250     False       False        True\n",
       "3         1  35.0      1      0  53.1000     False       False        True\n",
       "4         3  35.0      0      0   8.0500      True       False        True\n",
       "..      ...   ...    ...    ...      ...       ...         ...         ...\n",
       "886       2  27.0      0      0  13.0000      True       False        True\n",
       "887       1  19.0      0      0  30.0000     False       False        True\n",
       "888       3  28.0      1      2  23.4500     False       False        True\n",
       "889       1  26.0      0      0  30.0000      True       False       False\n",
       "890       3  32.0      0      0   7.7500      True        True       False\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, drop_first=True)\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45fe9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.select_dtypes(bool).columns] = data.select_dtypes(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c97863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(data, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb64e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov_mat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8937733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.47142396e+03, 1.67973620e+02, 1.26174262e+00, 4.72133896e-01,\n",
       "       4.25252370e-01, 4.60111600e-02, 1.76967025e-01, 2.41546779e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc36a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA : \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components \n",
    "        self.normalized_data = None\n",
    "        self.mean = None  \n",
    "        self.std = None\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None \n",
    "        self.components = None\n",
    "    \n",
    "\n",
    "    def fit(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        normalized_data = data - self.mean \n",
    "        self.std = np.std(data, axis=0) \n",
    "        self.normalized_data = normalized_data / self.std  \n",
    "\n",
    "        cov = np.cov(self.normalized_data, rowvar=False);  \n",
    "        # or cov = (self.normalized_data.T) \n",
    "\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov) ;  \n",
    "        # each eigen vector is a column in eigenvectors \n",
    "        # eigenvalues[i] corresponds to eigenvectors[:, i]\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        self.eigenvalues = eigenvalues[sorted_indices]\n",
    "        self.eigenvectors = eigenvectors[:, sorted_indices] \n",
    "        self.components = self.eigenvectors[:, :self.n_components] \n",
    "\n",
    "    def transform(self, data):  \n",
    "        data = data - self.mean \n",
    "        data = data / self.std  \n",
    "        return np.dot(data, self.components)  \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bdfde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 8 \n",
    "PCA_model = PCA(n_components=components) \n",
    "PCA_model .fit(data) \n",
    "pca_data = PCA_model.transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a95d6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = target.to_numpy()  \n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19323c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ratio = 0.2\n",
    "n_samples = pca_data.shape[0]\n",
    "indices = np.arange(n_samples) \n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "test_size = int(test_ratio * n_samples)\n",
    "\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pca_data[train_indices]\n",
    "y_train = target[train_indices] \n",
    "x_test = pca_data[test_indices]\n",
    "y_test = target[test_indices]\n",
    "\n",
    "\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train_std = np.std(x_train, axis=0)\n",
    "x_train = (x_train - x_train_mean) / x_train_std\n",
    "x_test = (x_test - x_train_mean) / x_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20988148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train.shape[1] \n",
    "def initialise_weights(x_train) : \n",
    "    w =np.zeros(x_train.shape[1])\n",
    "    b=0\n",
    "    return w,b   \n",
    "\n",
    "def linear_reg(x,w,b) : \n",
    "    return np.dot(x,w)+b  \n",
    "\n",
    "def sigmoid(z) : \n",
    "    \n",
    "  \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def compute_cost(y_train , y_pred) :  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cost = (-1/y_train.shape[0]) * np.sum(y_train*np.log(y_pred) + (1-y_train)*np.log(1-y_pred))\n",
    "    return cost\n",
    "\n",
    "def gradiant_descent(X,y_pred,y_true,weights, bias, learning_rate) :  \n",
    "    m=X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_pred - y_true))\n",
    "    db = (1/m) * np.sum(y_pred - y_true)\n",
    "    \n",
    "   \n",
    "    weights = weights - learning_rate * dw\n",
    "    bias = bias - learning_rate * db\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2ffb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations : 0.6931471805599453\n",
      "Cost after 10 iterations : 0.6476737513002577\n",
      "Cost after 20 iterations : 0.6119370758061798\n",
      "Cost after 30 iterations : 0.5836988980911088\n",
      "Cost after 40 iterations : 0.5612206034740904\n",
      "Cost after 50 iterations : 0.5431785687902596\n",
      "Cost after 60 iterations : 0.5285732156561599\n",
      "Cost after 70 iterations : 0.5166502112811667\n",
      "Cost after 80 iterations : 0.5068382050274209\n",
      "Cost after 90 iterations : 0.4987017652449445\n",
      "Cost after 100 iterations : 0.4919065472313969\n",
      "Cost after 110 iterations : 0.4861937316795018\n",
      "Cost after 120 iterations : 0.48136129148959994\n",
      "Cost after 130 iterations : 0.4772502241263826\n",
      "Cost after 140 iterations : 0.47373438016276403\n",
      "Cost after 150 iterations : 0.4707128987961389\n",
      "Cost after 160 iterations : 0.4681045406990933\n",
      "Cost after 170 iterations : 0.4658434096256986\n",
      "Cost after 180 iterations : 0.4638756973471846\n",
      "Cost after 190 iterations : 0.46215718810866924\n",
      "Cost after 200 iterations : 0.4606513310158282\n",
      "Cost after 210 iterations : 0.4593277402722907\n",
      "Cost after 220 iterations : 0.4581610201237448\n",
      "Cost after 230 iterations : 0.4571298380111289\n",
      "Cost after 240 iterations : 0.45621618878739856\n",
      "Cost after 250 iterations : 0.45540480700448027\n",
      "Cost after 260 iterations : 0.4546826946991116\n",
      "Cost after 270 iterations : 0.4540387398350765\n",
      "Cost after 280 iterations : 0.45346340633032056\n",
      "Cost after 290 iterations : 0.45294848093548395\n",
      "Cost after 300 iterations : 0.4524868655125996\n",
      "Cost after 310 iterations : 0.45207240576177293\n",
      "Cost after 320 iterations : 0.451699749358053\n",
      "Cost after 330 iterations : 0.4513642279358577\n",
      "Cost after 340 iterations : 0.45106175850144986\n",
      "Cost after 350 iterations : 0.45078876074465535\n",
      "Cost after 360 iterations : 0.45054208741865437\n",
      "Cost after 370 iterations : 0.4503189655058848\n",
      "Cost after 380 iterations : 0.4501169463225577\n",
      "Cost after 390 iterations : 0.44993386305959093\n",
      "Cost after 400 iterations : 0.4497677945334616\n",
      "Cost after 410 iterations : 0.4496170341415572\n",
      "Cost after 420 iterations : 0.4494800631946304\n",
      "Cost after 430 iterations : 0.4493555279429266\n",
      "Cost after 440 iterations : 0.4492422197294\n",
      "Cost after 450 iterations : 0.4491390577986711\n",
      "Cost after 460 iterations : 0.44904507436825947\n",
      "Cost after 470 iterations : 0.44895940163256615\n",
      "Cost after 480 iterations : 0.44888126042273824\n",
      "Cost after 490 iterations : 0.4488099502890827\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "learning_rate = 0.05  \n",
    "cost_history_train = []\n",
    "\n",
    "w,b = initialise_weights(x_train)\n",
    "for i in range(iterations) : \n",
    "    z_pred = linear_reg(x_train,w,b) \n",
    "    y_pred = sigmoid(z_pred) \n",
    "    cost = compute_cost(y_train,y_pred) \n",
    "    cost_history_train.append(cost)\n",
    "    w,b = gradiant_descent(x_train,y_pred,y_train,w,b,learning_rate) \n",
    "    if i%10 == 0 : \n",
    "        print(f\"Cost after {i} iterations : {cost}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44f937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42009805214708296\n"
     ]
    }
   ],
   "source": [
    "z_pred_test = linear_reg(x_test,w,b) \n",
    "y_pred_test = sigmoid(z_pred_test) \n",
    "cost_test = compute_cost(y_test,y_pred_test) \n",
    "print(cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d1587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15d098e2a50>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(iterations),cost_history_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cf76e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11858063, 0.22668219, 0.14189954, 0.88091271, 0.74499884,\n",
       "       0.9259526 , 0.68750913, 0.09967019, 0.75672096, 0.91536081,\n",
       "       0.33422712, 0.07642879, 0.41473986, 0.154687  , 0.20330946,\n",
       "       0.93157413, 0.30266155, 0.68754786, 0.24861309, 0.32951548,\n",
       "       0.12840933, 0.40295783, 0.62916968, 0.14226018, 0.11758563,\n",
       "       0.07567565, 0.45706325, 0.22768113, 0.088712  , 0.61116126,\n",
       "       0.14544148, 0.67077077, 0.54509843, 0.63136455, 0.14910943,\n",
       "       0.1341092 , 0.45025415, 0.68750913, 0.93662722, 0.11603503,\n",
       "       0.20835994, 0.0885764 , 0.11607103, 0.14419356, 0.58440906,\n",
       "       0.09663875, 0.14551575, 0.13202008, 0.12819405, 0.32641411,\n",
       "       0.72441491, 0.80620984, 0.04494924, 0.49646344, 0.05475365,\n",
       "       0.92630268, 0.22166354, 0.92379791, 0.77742355, 0.74048831,\n",
       "       0.13838125, 0.85425577, 0.76750364, 0.44486623, 0.14419356,\n",
       "       0.64381496, 0.31105557, 0.11310301, 0.15358485, 0.86945428,\n",
       "       0.75734947, 0.94261678, 0.55897816, 0.89739756, 0.13166299,\n",
       "       0.07489725, 0.67180431, 0.93926919, 0.75876787, 0.49104848,\n",
       "       0.05705634, 0.80217765, 0.90522072, 0.14419005, 0.34816513,\n",
       "       0.26557987, 0.92968214, 0.94004102, 0.3614655 , 0.11588436,\n",
       "       0.15695542, 0.45846936, 0.23881909, 0.14418772, 0.11603503,\n",
       "       0.11602532, 0.2671639 , 0.07441657, 0.76917355, 0.11902788,\n",
       "       0.22095297, 0.09671413, 0.94672732, 0.08939686, 0.1088718 ,\n",
       "       0.09849549, 0.68217584, 0.32276327, 0.11060377, 0.45891585,\n",
       "       0.89449212, 0.1399667 , 0.93171764, 0.38901191, 0.55584263,\n",
       "       0.14243267, 0.25904031, 0.27397281, 0.77910506, 0.35682225,\n",
       "       0.16145533, 0.93926716, 0.76612327, 0.36772748, 0.12505321,\n",
       "       0.4923631 , 0.85093609, 0.60029726, 0.66271915, 0.13845589,\n",
       "       0.6875723 , 0.11858063, 0.23785664, 0.77082898, 0.34632432,\n",
       "       0.74088409, 0.94944282, 0.11308019, 0.0805193 , 0.64110169,\n",
       "       0.13855998, 0.6753656 , 0.24326163, 0.16316935, 0.48770659,\n",
       "       0.71547769, 0.23279527, 0.09646446, 0.93957655, 0.0784908 ,\n",
       "       0.16284592, 0.12830748, 0.14903604, 0.63552629, 0.11607103,\n",
       "       0.11583773, 0.12787079, 0.68752747, 0.72380435, 0.57617814,\n",
       "       0.18083176, 0.44521754, 0.18964654, 0.93070688, 0.1443926 ,\n",
       "       0.28304411, 0.26967264, 0.89446258, 0.13500843, 0.09669922,\n",
       "       0.57094963, 0.81690506, 0.389066  , 0.752452  , 0.15253556,\n",
       "       0.16238493, 0.36935798, 0.85147872])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93744159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_true, y_pred): \n",
    "    \n",
    "    TP = TN = FP = FN = 0 \n",
    "    y_pred = np.where(y_pred>= 0.5, 1, 0)\n",
    "\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            TP += 1\n",
    "        elif yt == 0 and yp == 0:\n",
    "            TN += 1\n",
    "        elif yt == 0 and yp == 1:\n",
    "            FP += 1\n",
    "        elif yt == 1 and yp == 0:\n",
    "            FN += 1\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0 \n",
    "    print(\"TP:\", TP, \"FP:\", FP, \"FN:\", FN, \"TN:\", TN)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bceff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd7fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 52 FP: 15 FN: 21 TN: 90\n",
      "0.797752808988764 0.7761194029850746 0.7123287671232876 0.7428571428571429\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = classification_metrics(y_test, y_pred_test) \n",
    "print(accuracy, precision, recall, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a547c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred): \n",
    "    n_samples = y_true.shape[0]\n",
    "    distances = 1 - y_true * y_pred\n",
    "    distances = np.maximum(0, distances)\n",
    "    hinge_loss = np.mean(distances)\n",
    "    return hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3685a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lamdbda_param=0.01, n_iters=1500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamdbda_param = lamdbda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) + self.bias) >= 1\n",
    "\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (\n",
    "                        2 * self.lamdbda_param * self.weights\n",
    "                    )\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (\n",
    "                        2 * self.lamdbda_param * self.weights - y[idx] * x_i\n",
    "                    )\n",
    "                    self.bias += self.learning_rate * y[idx]\n",
    "\n",
    "            if iteration % 1 == 0:\n",
    "                margins = 1 - y * (X @ self.weights + self.bias)\n",
    "                loss = self.lamdbda_param * np.dot(self.weights, self.weights) \\\n",
    "                       + np.mean(np.maximum(0, margins))\n",
    "                print(f\"Iter {iteration}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) + self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05f3d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Loss = 0.7134\n",
      "Iter 1: Loss = 0.5088\n",
      "Iter 2: Loss = 0.4684\n",
      "Iter 3: Loss = 0.4494\n",
      "Iter 4: Loss = 0.4406\n",
      "Iter 5: Loss = 0.4384\n",
      "Iter 6: Loss = 0.4373\n",
      "Iter 7: Loss = 0.4367\n",
      "Iter 8: Loss = 0.4365\n",
      "Iter 9: Loss = 0.4362\n",
      "Iter 10: Loss = 0.4361\n",
      "Iter 11: Loss = 0.4359\n",
      "Iter 12: Loss = 0.4357\n",
      "Iter 13: Loss = 0.4357\n",
      "Iter 14: Loss = 0.4354\n",
      "Iter 15: Loss = 0.4354\n",
      "Iter 16: Loss = 0.4352\n",
      "Iter 17: Loss = 0.4351\n",
      "Iter 18: Loss = 0.4351\n",
      "Iter 19: Loss = 0.4348\n",
      "Iter 20: Loss = 0.4349\n",
      "Iter 21: Loss = 0.4347\n",
      "Iter 22: Loss = 0.4347\n",
      "Iter 23: Loss = 0.4348\n",
      "Iter 24: Loss = 0.4346\n",
      "Iter 25: Loss = 0.4346\n",
      "Iter 26: Loss = 0.4346\n",
      "Iter 27: Loss = 0.4346\n",
      "Iter 28: Loss = 0.4346\n",
      "Iter 29: Loss = 0.4347\n",
      "Iter 30: Loss = 0.4345\n",
      "Iter 31: Loss = 0.4344\n",
      "Iter 32: Loss = 0.4347\n",
      "Iter 33: Loss = 0.4348\n",
      "Iter 34: Loss = 0.4347\n",
      "Iter 35: Loss = 0.4346\n",
      "Iter 36: Loss = 0.4347\n",
      "Iter 37: Loss = 0.4345\n",
      "Iter 38: Loss = 0.4346\n",
      "Iter 39: Loss = 0.4345\n",
      "Iter 40: Loss = 0.4345\n",
      "Iter 41: Loss = 0.4346\n",
      "Iter 42: Loss = 0.4347\n",
      "Iter 43: Loss = 0.4347\n",
      "Iter 44: Loss = 0.4345\n",
      "Iter 45: Loss = 0.4347\n",
      "Iter 46: Loss = 0.4346\n",
      "Iter 47: Loss = 0.4347\n",
      "Iter 48: Loss = 0.4345\n",
      "Iter 49: Loss = 0.4348\n",
      "Iter 50: Loss = 0.4347\n",
      "Iter 51: Loss = 0.4348\n",
      "Iter 52: Loss = 0.4347\n",
      "Iter 53: Loss = 0.4345\n",
      "Iter 54: Loss = 0.4346\n",
      "Iter 55: Loss = 0.4347\n",
      "Iter 56: Loss = 0.4347\n",
      "Iter 57: Loss = 0.4346\n",
      "Iter 58: Loss = 0.4345\n",
      "Iter 59: Loss = 0.4345\n",
      "Iter 60: Loss = 0.4345\n",
      "Iter 61: Loss = 0.4347\n",
      "Iter 62: Loss = 0.4345\n",
      "Iter 63: Loss = 0.4346\n",
      "Iter 64: Loss = 0.4346\n",
      "Iter 65: Loss = 0.4348\n",
      "Iter 66: Loss = 0.4347\n",
      "Iter 67: Loss = 0.4347\n",
      "Iter 68: Loss = 0.4345\n",
      "Iter 69: Loss = 0.4346\n",
      "Iter 70: Loss = 0.4347\n",
      "Iter 71: Loss = 0.4345\n",
      "Iter 72: Loss = 0.4344\n",
      "Iter 73: Loss = 0.4347\n",
      "Iter 74: Loss = 0.4345\n",
      "Iter 75: Loss = 0.4345\n",
      "Iter 76: Loss = 0.4348\n",
      "Iter 77: Loss = 0.4345\n",
      "Iter 78: Loss = 0.4346\n",
      "Iter 79: Loss = 0.4346\n",
      "Iter 80: Loss = 0.4347\n",
      "Iter 81: Loss = 0.4348\n",
      "Iter 82: Loss = 0.4345\n",
      "Iter 83: Loss = 0.4345\n",
      "Iter 84: Loss = 0.4347\n",
      "Iter 85: Loss = 0.4346\n",
      "Iter 86: Loss = 0.4345\n",
      "Iter 87: Loss = 0.4346\n",
      "Iter 88: Loss = 0.4347\n",
      "Iter 89: Loss = 0.4345\n",
      "Iter 90: Loss = 0.4345\n",
      "Iter 91: Loss = 0.4346\n",
      "Iter 92: Loss = 0.4345\n",
      "Iter 93: Loss = 0.4346\n",
      "Iter 94: Loss = 0.4344\n",
      "Iter 95: Loss = 0.4345\n",
      "Iter 96: Loss = 0.4344\n",
      "Iter 97: Loss = 0.4346\n",
      "Iter 98: Loss = 0.4346\n",
      "Iter 99: Loss = 0.4348\n",
      "Iter 100: Loss = 0.4346\n",
      "Iter 101: Loss = 0.4346\n",
      "Iter 102: Loss = 0.4348\n",
      "Iter 103: Loss = 0.4346\n",
      "Iter 104: Loss = 0.4346\n",
      "Iter 105: Loss = 0.4345\n",
      "Iter 106: Loss = 0.4346\n",
      "Iter 107: Loss = 0.4346\n",
      "Iter 108: Loss = 0.4346\n",
      "Iter 109: Loss = 0.4347\n",
      "Iter 110: Loss = 0.4345\n",
      "Iter 111: Loss = 0.4345\n",
      "Iter 112: Loss = 0.4345\n",
      "Iter 113: Loss = 0.4346\n",
      "Iter 114: Loss = 0.4346\n",
      "Iter 115: Loss = 0.4345\n",
      "Iter 116: Loss = 0.4349\n",
      "Iter 117: Loss = 0.4347\n",
      "Iter 118: Loss = 0.4346\n",
      "Iter 119: Loss = 0.4345\n",
      "Iter 120: Loss = 0.4347\n",
      "Iter 121: Loss = 0.4346\n",
      "Iter 122: Loss = 0.4345\n",
      "Iter 123: Loss = 0.4347\n",
      "Iter 124: Loss = 0.4345\n",
      "Iter 125: Loss = 0.4348\n",
      "Iter 126: Loss = 0.4346\n",
      "Iter 127: Loss = 0.4346\n",
      "Iter 128: Loss = 0.4345\n",
      "Iter 129: Loss = 0.4345\n",
      "Iter 130: Loss = 0.4347\n",
      "Iter 131: Loss = 0.4348\n",
      "Iter 132: Loss = 0.4346\n",
      "Iter 133: Loss = 0.4346\n",
      "Iter 134: Loss = 0.4347\n",
      "Iter 135: Loss = 0.4346\n",
      "Iter 136: Loss = 0.4348\n",
      "Iter 137: Loss = 0.4345\n",
      "Iter 138: Loss = 0.4346\n",
      "Iter 139: Loss = 0.4346\n",
      "Iter 140: Loss = 0.4348\n",
      "Iter 141: Loss = 0.4347\n",
      "Iter 142: Loss = 0.4346\n",
      "Iter 143: Loss = 0.4349\n",
      "Iter 144: Loss = 0.4346\n",
      "Iter 145: Loss = 0.4346\n",
      "Iter 146: Loss = 0.4346\n",
      "Iter 147: Loss = 0.4347\n",
      "Iter 148: Loss = 0.4348\n",
      "Iter 149: Loss = 0.4346\n",
      "Iter 150: Loss = 0.4348\n",
      "Iter 151: Loss = 0.4345\n",
      "Iter 152: Loss = 0.4344\n",
      "Iter 153: Loss = 0.4347\n",
      "Iter 154: Loss = 0.4346\n",
      "Iter 155: Loss = 0.4347\n",
      "Iter 156: Loss = 0.4347\n",
      "Iter 157: Loss = 0.4347\n",
      "Iter 158: Loss = 0.4345\n",
      "Iter 159: Loss = 0.4346\n",
      "Iter 160: Loss = 0.4347\n",
      "Iter 161: Loss = 0.4344\n",
      "Iter 162: Loss = 0.4348\n",
      "Iter 163: Loss = 0.4348\n",
      "Iter 164: Loss = 0.4345\n",
      "Iter 165: Loss = 0.4345\n",
      "Iter 166: Loss = 0.4345\n",
      "Iter 167: Loss = 0.4346\n",
      "Iter 168: Loss = 0.4347\n",
      "Iter 169: Loss = 0.4346\n",
      "Iter 170: Loss = 0.4346\n",
      "Iter 171: Loss = 0.4346\n",
      "Iter 172: Loss = 0.4347\n",
      "Iter 173: Loss = 0.4346\n",
      "Iter 174: Loss = 0.4345\n",
      "Iter 175: Loss = 0.4346\n",
      "Iter 176: Loss = 0.4347\n",
      "Iter 177: Loss = 0.4345\n",
      "Iter 178: Loss = 0.4344\n",
      "Iter 179: Loss = 0.4346\n",
      "Iter 180: Loss = 0.4347\n",
      "Iter 181: Loss = 0.4345\n",
      "Iter 182: Loss = 0.4346\n",
      "Iter 183: Loss = 0.4348\n",
      "Iter 184: Loss = 0.4346\n",
      "Iter 185: Loss = 0.4347\n",
      "Iter 186: Loss = 0.4347\n",
      "Iter 187: Loss = 0.4345\n",
      "Iter 188: Loss = 0.4345\n",
      "Iter 189: Loss = 0.4345\n",
      "Iter 190: Loss = 0.4347\n",
      "Iter 191: Loss = 0.4347\n",
      "Iter 192: Loss = 0.4345\n",
      "Iter 193: Loss = 0.4347\n",
      "Iter 194: Loss = 0.4344\n",
      "Iter 195: Loss = 0.4346\n",
      "Iter 196: Loss = 0.4346\n",
      "Iter 197: Loss = 0.4347\n",
      "Iter 198: Loss = 0.4345\n",
      "Iter 199: Loss = 0.4349\n",
      "Iter 200: Loss = 0.4345\n",
      "Iter 201: Loss = 0.4345\n",
      "Iter 202: Loss = 0.4346\n",
      "Iter 203: Loss = 0.4347\n",
      "Iter 204: Loss = 0.4347\n",
      "Iter 205: Loss = 0.4348\n",
      "Iter 206: Loss = 0.4346\n",
      "Iter 207: Loss = 0.4345\n",
      "Iter 208: Loss = 0.4345\n",
      "Iter 209: Loss = 0.4348\n",
      "Iter 210: Loss = 0.4346\n",
      "Iter 211: Loss = 0.4347\n",
      "Iter 212: Loss = 0.4345\n",
      "Iter 213: Loss = 0.4347\n",
      "Iter 214: Loss = 0.4347\n",
      "Iter 215: Loss = 0.4347\n",
      "Iter 216: Loss = 0.4346\n",
      "Iter 217: Loss = 0.4346\n",
      "Iter 218: Loss = 0.4345\n",
      "Iter 219: Loss = 0.4348\n",
      "Iter 220: Loss = 0.4347\n",
      "Iter 221: Loss = 0.4345\n",
      "Iter 222: Loss = 0.4347\n",
      "Iter 223: Loss = 0.4347\n",
      "Iter 224: Loss = 0.4347\n",
      "Iter 225: Loss = 0.4346\n",
      "Iter 226: Loss = 0.4347\n",
      "Iter 227: Loss = 0.4346\n",
      "Iter 228: Loss = 0.4346\n",
      "Iter 229: Loss = 0.4347\n",
      "Iter 230: Loss = 0.4346\n",
      "Iter 231: Loss = 0.4348\n",
      "Iter 232: Loss = 0.4345\n",
      "Iter 233: Loss = 0.4344\n",
      "Iter 234: Loss = 0.4343\n",
      "Iter 235: Loss = 0.4344\n",
      "Iter 236: Loss = 0.4346\n",
      "Iter 237: Loss = 0.4346\n",
      "Iter 238: Loss = 0.4345\n",
      "Iter 239: Loss = 0.4344\n",
      "Iter 240: Loss = 0.4347\n",
      "Iter 241: Loss = 0.4344\n",
      "Iter 242: Loss = 0.4345\n",
      "Iter 243: Loss = 0.4345\n",
      "Iter 244: Loss = 0.4345\n",
      "Iter 245: Loss = 0.4346\n",
      "Iter 246: Loss = 0.4347\n",
      "Iter 247: Loss = 0.4347\n",
      "Iter 248: Loss = 0.4344\n",
      "Iter 249: Loss = 0.4345\n",
      "Iter 250: Loss = 0.4346\n",
      "Iter 251: Loss = 0.4346\n",
      "Iter 252: Loss = 0.4346\n",
      "Iter 253: Loss = 0.4346\n",
      "Iter 254: Loss = 0.4348\n",
      "Iter 255: Loss = 0.4348\n",
      "Iter 256: Loss = 0.4348\n",
      "Iter 257: Loss = 0.4344\n",
      "Iter 258: Loss = 0.4346\n",
      "Iter 259: Loss = 0.4346\n",
      "Iter 260: Loss = 0.4345\n",
      "Iter 261: Loss = 0.4348\n",
      "Iter 262: Loss = 0.4346\n",
      "Iter 263: Loss = 0.4348\n",
      "Iter 264: Loss = 0.4347\n",
      "Iter 265: Loss = 0.4348\n",
      "Iter 266: Loss = 0.4346\n",
      "Iter 267: Loss = 0.4347\n",
      "Iter 268: Loss = 0.4348\n",
      "Iter 269: Loss = 0.4345\n",
      "Iter 270: Loss = 0.4345\n",
      "Iter 271: Loss = 0.4347\n",
      "Iter 272: Loss = 0.4349\n",
      "Iter 273: Loss = 0.4347\n",
      "Iter 274: Loss = 0.4345\n",
      "Iter 275: Loss = 0.4347\n",
      "Iter 276: Loss = 0.4346\n",
      "Iter 277: Loss = 0.4347\n",
      "Iter 278: Loss = 0.4346\n",
      "Iter 279: Loss = 0.4346\n",
      "Iter 280: Loss = 0.4348\n",
      "Iter 281: Loss = 0.4346\n",
      "Iter 282: Loss = 0.4347\n",
      "Iter 283: Loss = 0.4345\n",
      "Iter 284: Loss = 0.4344\n",
      "Iter 285: Loss = 0.4346\n",
      "Iter 286: Loss = 0.4346\n",
      "Iter 287: Loss = 0.4346\n",
      "Iter 288: Loss = 0.4347\n",
      "Iter 289: Loss = 0.4345\n",
      "Iter 290: Loss = 0.4345\n",
      "Iter 291: Loss = 0.4346\n",
      "Iter 292: Loss = 0.4347\n",
      "Iter 293: Loss = 0.4344\n",
      "Iter 294: Loss = 0.4346\n",
      "Iter 295: Loss = 0.4345\n",
      "Iter 296: Loss = 0.4347\n",
      "Iter 297: Loss = 0.4346\n",
      "Iter 298: Loss = 0.4344\n",
      "Iter 299: Loss = 0.4346\n",
      "Iter 300: Loss = 0.4348\n",
      "Iter 301: Loss = 0.4347\n",
      "Iter 302: Loss = 0.4347\n",
      "Iter 303: Loss = 0.4347\n",
      "Iter 304: Loss = 0.4347\n",
      "Iter 305: Loss = 0.4347\n",
      "Iter 306: Loss = 0.4348\n",
      "Iter 307: Loss = 0.4344\n",
      "Iter 308: Loss = 0.4347\n",
      "Iter 309: Loss = 0.4345\n",
      "Iter 310: Loss = 0.4344\n",
      "Iter 311: Loss = 0.4344\n",
      "Iter 312: Loss = 0.4346\n",
      "Iter 313: Loss = 0.4346\n",
      "Iter 314: Loss = 0.4348\n",
      "Iter 315: Loss = 0.4345\n",
      "Iter 316: Loss = 0.4347\n",
      "Iter 317: Loss = 0.4347\n",
      "Iter 318: Loss = 0.4346\n",
      "Iter 319: Loss = 0.4346\n",
      "Iter 320: Loss = 0.4346\n",
      "Iter 321: Loss = 0.4344\n",
      "Iter 322: Loss = 0.4347\n",
      "Iter 323: Loss = 0.4346\n",
      "Iter 324: Loss = 0.4346\n",
      "Iter 325: Loss = 0.4345\n",
      "Iter 326: Loss = 0.4345\n",
      "Iter 327: Loss = 0.4347\n",
      "Iter 328: Loss = 0.4347\n",
      "Iter 329: Loss = 0.4345\n",
      "Iter 330: Loss = 0.4347\n",
      "Iter 331: Loss = 0.4345\n",
      "Iter 332: Loss = 0.4346\n",
      "Iter 333: Loss = 0.4348\n",
      "Iter 334: Loss = 0.4347\n",
      "Iter 335: Loss = 0.4346\n",
      "Iter 336: Loss = 0.4346\n",
      "Iter 337: Loss = 0.4346\n",
      "Iter 338: Loss = 0.4348\n",
      "Iter 339: Loss = 0.4347\n",
      "Iter 340: Loss = 0.4346\n",
      "Iter 341: Loss = 0.4348\n",
      "Iter 342: Loss = 0.4347\n",
      "Iter 343: Loss = 0.4346\n",
      "Iter 344: Loss = 0.4345\n",
      "Iter 345: Loss = 0.4347\n",
      "Iter 346: Loss = 0.4348\n",
      "Iter 347: Loss = 0.4346\n",
      "Iter 348: Loss = 0.4345\n",
      "Iter 349: Loss = 0.4345\n",
      "Iter 350: Loss = 0.4347\n",
      "Iter 351: Loss = 0.4347\n",
      "Iter 352: Loss = 0.4345\n",
      "Iter 353: Loss = 0.4346\n",
      "Iter 354: Loss = 0.4345\n",
      "Iter 355: Loss = 0.4347\n",
      "Iter 356: Loss = 0.4346\n",
      "Iter 357: Loss = 0.4344\n",
      "Iter 358: Loss = 0.4346\n",
      "Iter 359: Loss = 0.4346\n",
      "Iter 360: Loss = 0.4347\n",
      "Iter 361: Loss = 0.4347\n",
      "Iter 362: Loss = 0.4346\n",
      "Iter 363: Loss = 0.4345\n",
      "Iter 364: Loss = 0.4344\n",
      "Iter 365: Loss = 0.4347\n",
      "Iter 366: Loss = 0.4348\n",
      "Iter 367: Loss = 0.4346\n",
      "Iter 368: Loss = 0.4344\n",
      "Iter 369: Loss = 0.4347\n",
      "Iter 370: Loss = 0.4346\n",
      "Iter 371: Loss = 0.4345\n",
      "Iter 372: Loss = 0.4345\n",
      "Iter 373: Loss = 0.4346\n",
      "Iter 374: Loss = 0.4347\n",
      "Iter 375: Loss = 0.4347\n",
      "Iter 376: Loss = 0.4347\n",
      "Iter 377: Loss = 0.4346\n",
      "Iter 378: Loss = 0.4347\n",
      "Iter 379: Loss = 0.4346\n",
      "Iter 380: Loss = 0.4346\n",
      "Iter 381: Loss = 0.4345\n",
      "Iter 382: Loss = 0.4344\n",
      "Iter 383: Loss = 0.4346\n",
      "Iter 384: Loss = 0.4346\n",
      "Iter 385: Loss = 0.4346\n",
      "Iter 386: Loss = 0.4345\n",
      "Iter 387: Loss = 0.4346\n",
      "Iter 388: Loss = 0.4347\n",
      "Iter 389: Loss = 0.4346\n",
      "Iter 390: Loss = 0.4348\n",
      "Iter 391: Loss = 0.4346\n",
      "Iter 392: Loss = 0.4347\n",
      "Iter 393: Loss = 0.4346\n",
      "Iter 394: Loss = 0.4347\n",
      "Iter 395: Loss = 0.4347\n",
      "Iter 396: Loss = 0.4346\n",
      "Iter 397: Loss = 0.4346\n",
      "Iter 398: Loss = 0.4347\n",
      "Iter 399: Loss = 0.4347\n"
     ]
    }
   ],
   "source": [
    "SVM_model = SVM(learning_rate=0.001, lamdbda_param=0.01, n_iters=400) \n",
    "SVM_model.fit(x_train, y_train)\n",
    "y_pred = SVM_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bdf8ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1], dtype=int64),\n",
       " array([-1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "        -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "        -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
       "        -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
       "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,\n",
       "         1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
       "         1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
       "         1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
       "        -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "        -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
       "         1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
       "        -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,\n",
       "        -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
       "        -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d0b991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics_svm(y_true, y_pred):\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            TP += 1\n",
    "        elif yt == -1 and yp == -1:\n",
    "            TN += 1\n",
    "        elif yt == -1 and yp == 1:\n",
    "            FP += 1\n",
    "        elif yt == 1 and yp == -1:\n",
    "            FN += 1\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    print(\"TP:\", TP, \"FP:\", FP, \"FN:\", FN, \"TN:\", TN)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e51fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 51 FP: 17 FN: 22 TN: 88\n",
      "0.7808988764044944 0.75 0.6986301369863014 0.7234042553191489\n",
      "0.44975462317338466\n"
     ]
    }
   ],
   "source": [
    "def svm_loss(X, y, w, b, lambda_param):\n",
    "    \n",
    "    y_converted = np.where(y <= 0, -1, 1)\n",
    "    margins = 1 - y_converted * (np.dot(X, w) + b)\n",
    "    hinge_loss = np.maximum(0, margins)\n",
    "    return lambda_param * np.dot(w, w) + np.mean(hinge_loss) \n",
    "\n",
    "y_test = np.where(y_test <= 0, -1, 1) \n",
    "accuracy, precision, recall, f1 = classification_metrics_svm(y_test, y_pred) \n",
    "print(accuracy, precision, recall, f1)\n",
    "test_loss = svm_loss(x_test, y_test, SVM_model.weights, SVM_model.bias, SVM_model.lamdbda_param) \n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123f387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
